---
jurisdiction: EU
law: DSA
effective_start: 2024-02-17
source_url: https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32022R2065
---

# Digital Services Act (extract)

## Article 28 - Risk assessment for very large online platforms and very large online search engines

1. Providers of very large online platforms and very large online search engines shall diligently identify, analyse and assess any systemic risks stemming from the functioning and use of their service in the Union, that may arise, in particular:
   
   (a) from the way in which their algorithmic systems amplify certain risks, such as:
       - illegal content in areas such as child sexual abuse, terrorist content, illegal hate speech, commercial fraud and counterfeiting of products;
       - risks for the exercise of fundamental rights, in particular the rights to human dignity, freedom of expression and information, the right to private and family life, the rights of the child and consumer protection, the right to non-discrimination and the rights of women and of persons belonging to vulnerable groups;
   
   (b) from their use by recipients of the service, including misuse through:
       - the submission of illegal content, including by inauthentic use or automated exploitation of the service;
       - inauthentic or abusive behavior that may lead to the rapid and wide dissemination of information that is illegal content or incompatible with their terms and conditions;

## Article 34 - Transparency obligations for recommender systems

1. Providers of online platforms that use recommender systems shall set out in their terms and conditions, in plain and intelligible language, the main parameters used in their recommender systems, as well as any options for the recipients of the service to modify or influence those main parameters.

## Article 35 - Additional risk mitigation measures for very large online platforms and very large online search engines

1. Where the risk assessment carried out in accordance with Article 28 reveals one or more systemic risks, providers of very large online platforms and very large online search engines shall put in place reasonable, proportionate and effective mitigation measures, tailored to the specific systemic risks identified.

2. The measures referred to in paragraph 1 may include, in particular:
   (a) adapting content moderation or other relevant systems, processes or technologies used by the provider;
   (b) adapting decision-making processes, features or functionalities of the service, or their operation;
   (c) requesting the implementation of safeguards for certain groups of recipients of the service, such as measures to ensure the protection of minors;
   (d) introducing targeted measures to limit the display of certain types of content;
   (e) introducing warning mechanisms;
   (f) adapting recommender systems or advertising systems;
   (g) improving the user reporting and redress systems.

## Article 16 - Orders to act against illegal content

1. Upon receipt of an order issued by the relevant judicial or administrative authorities of a Member State, based on the applicable Union or national law, in conformity with Union law, providers of intermediary services shall, without undue delay, inform the issuing authority of the receipt of the order and of the effect given to it.

2. The orders referred to in paragraph 1 shall contain at least the following elements:
   (a) a statement of reasons explaining why the information is considered to be illegal content, by reference to one or more specific provisions of Union law or national law in conformity with Union law;
   (b) one or more exact uniform resource locators (URLs) and, where necessary, additional information enabling the identification of the illegal content concerned;
   (c) information about redress available to the provider and to the recipient of the service whose content is affected by the order.

## Article 24 - Notification of illegal content

1. Providers of hosting services shall put in place mechanisms that allow any individual or entity to notify them of the presence on their service of specific items of information that the individual or entity considers to be illegal content.

2. The mechanisms referred to in paragraph 1 shall be user-friendly and allow for the submission of notices exclusively by electronic means. It shall be accessible and visible on the providers' online interface and allow for the submission of notices in at least one of the official languages of the Member State in which the provider is established and in at least one other language that is broadly understood by the largest possible number of speakers in the Union.

3. The mechanisms referred to in paragraph 1 shall be such as to facilitate the submission of sufficiently precise and adequately substantiated notices. For that purpose, the providers shall request that the notifications contain at least all of the following elements:
   (a) an explanation of the reasons why the notifier considers the information in question to be illegal content, specifying the relevant provisions of law;
   (b) the electronic location of the information, such as the exact URL or URLs, and, where necessary, additional information enabling the identification of the illegal content;
   (c) the name and electronic mail address of the notifier, except in the case of information considered to involve one of the offences referred to in Articles 3 to 7 of Directive 2011/93/EU;
   (d) a statement confirming the bona fide belief of the notifier that the information and allegations contained in the notification are accurate and complete.

## Article 25 - Processing of notices

1. Where a provider of hosting services receives a notice through the mechanisms referred to in Article 24(1), it shall, without undue delay, assess the notice and any accompanying material, and, where applicable, the relevant content to which the notice relates, in a timely, diligent, non-arbitrary and objective manner.

2. Where the provider decides to remove or disable access to the information that is the subject of the notice, it shall inform without undue delay the notifier and the provider of the content of that decision and the reasons therefor.

## Additional Context and Geographic Implementation

### Article 26 - Content Moderation Policies and Geographic Scope

**EU-Specific Requirements:**
- Content moderation must comply with Member State laws
- Recognition of national cultural and linguistic differences
- Coordination with national regulatory authorities
- Respect for fundamental rights as enshrined in EU law

**Geographic Enforcement:**
- Services must identify users located in EU territory
- Different content standards may apply based on user location
- Cross-border coordination between EU Member States required
- Non-EU platforms serving EU users must comply with DSA requirements

### Article 27 - Transparency Reporting for EU Operations

**Mandatory Reporting Elements:**
- Number of content removals by Member State
- Types of illegal content addressed in EU territory
- Response times for content moderation in different EU languages
- Coordination efforts with national authorities

**Geographic Data Requirements:**
- User location data for compliance purposes only
- Privacy-preserving geographic detection methods
- Data localization requirements for EU user information
- Cross-border data transfer restrictions within compliance framework

### Article 38 - Risk Management for Very Large Online Platforms

**EU-Specific Systemic Risk Assessment:**
- Impact assessment on fundamental rights in EU context
- Evaluation of risks to democratic processes and civic discourse
- Analysis of amplification of harmful content in EU languages
- Assessment of cross-border information manipulation risks

**Geographic Risk Mitigation:**
- Tailored content policies for different EU Member States
- Language-specific content moderation capabilities
- Cultural sensitivity in algorithmic content promotion
- Coordination with EU institutions on emerging threats

### Article 40 - External Auditing Requirements

**Independent Audit Framework:**
- Annual audits by EU-recognized auditing bodies
- Assessment of compliance with Member State laws
- Evaluation of fundamental rights impact assessments
- Review of cross-border cooperation mechanisms

**Audit Scope and Standards:**
- Technical compliance with DSA requirements
- Effectiveness of content moderation at scale
- Adequacy of user redress mechanisms
- Transparency and accountability of algorithmic systems

### Article 42 - Crisis Response Mechanism

**EU Crisis Coordination:**
- Rapid response to threats affecting multiple Member States
- Coordination with European External Action Service
- Integration with EU cybersecurity framework
- Support for Member State emergency response capabilities

**Geographic Crisis Management:**
- Location-aware threat detection systems
- Coordinated response across EU territory
- Information sharing with relevant national authorities
- Proportionate measures respecting national sovereignty

This framework ensures comprehensive digital services regulation across the European Union while respecting Member State sovereignty and fundamental rights.
   (a) adapting content moderation or other relevant systems, processes or mechanisms, including by adapting automated tools or increasing human oversight;
   (b) adapting decision-making processes;
   (c) adapting the design, features or functioning of their services or their app, including the recommender systems;
   (d) taking action related to advertising systems;
   (e) pursuing awareness-raising initiatives and adapting interfaces to enable user empowerment and literacy.

## Article 24 - Notification of illegal content

1. Providers of hosting services shall put in place mechanisms that allow any individual or entity to notify them of the presence on their service of specific items of information that the individual or entity considers to be illegal content.

2. The mechanisms referred to in paragraph 1 shall be user-friendly and allow for the submission of notices exclusively by electronic means. It shall be accessible and visible on the providers' online interface and allow for the submission of notices in at least one of the official languages of the Member State in which the provider is established and in at least one other language that is broadly understood by the largest possible number of speakers in the Union.

## Article 25 - Processing of notices

1. Upon receiving a notice through the mechanisms referred to in Article 24(1), providers of hosting services shall:
   (a) process the notice without undue delay;
   (b) decide whether the information to which the notice relates constitutes illegal content, in accordance with applicable Union law and national law;
   (c) where they decide that the information constitutes illegal content, act expeditiously to remove or to disable access to that information;
   (d) inform without undue delay the individual or entity that submitted the notice of their decision and provide a clear and specific statement of reasons for that decision.